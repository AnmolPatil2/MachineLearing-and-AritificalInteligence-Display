{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentimental analysis 1o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "hm_lines = 100000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5331\n",
      "115378\n"
     ]
    }
   ],
   "source": [
    "lexicon = []\n",
    "with open('pos.txt','r') as f:\n",
    "\tcontents = f.readlines()\n",
    "    \n",
    "\tfor l in contents[:hm_lines]:\n",
    "\t\tall_words = word_tokenize(l)\n",
    "\t\tlexicon += list(all_words)\n",
    "\n",
    "print(len(contents))\n",
    "print(len(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tlexicon = []\n",
    "\twith open('pos.txt','r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)\n",
    "            \n",
    "\twith open('neg.txt','r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230193"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rock', 'be', 'century', 'new', '``', 'he', 'going', 'make', 'even', 'than', 'arnold', 'van', 'or', 'steven', 'elaborate', 'ring', 'so', 'huge', 'word', 'can', 'not', 'peter', 'jackson', 'vision', 'j', 'effective', 'if', 'sometimes', 'like', 'go', 'have', 'fun', 'good', 'place', 'start', 'emerges', 'something', 'rare', 'issue', 'honest', 'doe', \"n't\", 'feel', 'one', 'provides', 'some', 'great', 'insight', 'into', 'all', 'comic', '--', 'those', 'who', 'top', 'game', 'offer', 'combination', 'entertainment', 'perhaps', 'no', 'picture', 'ever', 'made', 'ha', 'more', 'literally', 'road', 'hell', 'intention', 'turn', 'screenplay', 'at', 'edge', ';', 'clever', 'want', 'hate', 'somehow', 'pull', 'off', 'take', 'care', 'my', 'refreshingly', 'different', 'slice', 'cinema', 'well', 'worth', 'seeing', 'talking', 'head', 'what', 'really', 'surprise', 'about', 'low-key', 'quality', 'genuine', '(', ')', 'why', 'we', ':', 'through', 'eye', 'heart', 'mind', 'greatest', 'ultimately', 'reason', 'need', 'story', 'much', 'utterly', 'compelling', 'wrote', \"'\", 'which', 'most', 'famous', 'lived', 'come', 'question', 'overly', 'documentary', 'masterpiece', 'four', 'year', 'making', 'beauty', 'will', 'willing', 'mystery', 'fresh', 'air', 'true', 'thoughtful', 'provocative', 'cast', 'actor', 'working', 'lovely', '&', 'amazing', 'u', 'because', 'amusing', 'how', 'our', 'life', 'disturbing', 'evocative', 'imagery', 'music', 'by', 'glass', 'everyone', 'whom', 'connect', 'nice', 'from', 'standard', 'fare', 'score', 'few', 'point', 'doing', 'occasionally', 'melodramatic', 'also', 'extremely', 'love', 'brings', 'out', 'romantic', 'minute', 'treasure', 'planet', 'maintains', 'pace', 'race', 'familiar', 'however', 'lack', 'grandeur', 'epic', 'often', 'tale', 'earlier', 'disney', 'effort', 'help', 'bow', 'wow', 'tone', 'down', 'his', 'act', 'play', 'someone', 'resembles', 'real', 'kid', 'move', 'anyone', 'masterful', 'master', 'filmmaker', 'unique', 'light', 'cute', 'forgettable', 'there', 'way', 'effectively', 'drug', 'i', 'think', 'project', 'unfortunately', 'paid', 'while', 'would', 'easy', 'give', 'crush', 'title', 'two', 'wedding', 'far', 'any', 'hugh', 'grant', 'whimsy', 'though', 'everything', 'might', 'smart', 'never', 'took', 'always', 'seemed', 'static', 'perfectly', 'capture', 'vincent', 'day', 'm', 'almost', 'spooky', 'her', 'mean', 'best', 'work', 'distinctive', 'fascinating', 'had', 'them', 'bond', 'recent', 'stunt', 'are', 'they', 'on', 'being', 'heavy', 'technology', 'beginning', 'series', 'draw', 'attention', 'around', 'better', 'mark', 'loses', 'bite', 'happy', 'ending', 'le', 'rest', 'novel', 'ride', 'surely', 'called', 'time', 'exhilarating', 'yarn', 'e', 'el', 'de', 'que', 'la', 'strong', 'case', 'importance', 'creating', 'sound', 'rhythm', 'itself', 'beat', 'long', 'hair', 'little', 'away', 'performance', 'else', 'people', 'glimpse', 'culture', 'do', 'know', 'tender', 'yet', 'darkly', 'funny', 'fable', 'may', 'target', 'old', 'giant', 'creature', 'feature', 'celebrates', 'their', 'get', 'kick', 'watching', 'today', 'engaging', 'eccentric', 'career', 'cheap', 'charisma', \"'d\", 'reading', 'phone', 'book', 'bullock', 'such', 'beautifully', 'chilly', 'martha', 'begin', 'open', 'behind', 'snow', 'dog', 'brother', 'run', 'find', 'manages', 'original', 'many', 'idea', 'adam', 'song', '\\x97', 'potential', 'hit', 'simply', 'whole', 'certainly', 'intended', 'spirit', 'piece', 'now', 'america', 'enough', 'british', 'still', 'charming', 'here', 'whether', \"'re\", 'other', 'undeniably', 'playful', 'pleasant', 'held', 'together', 'ensemble', 'american', 'teen', 'since', 'whatever', 'been', 'when', 'bringing', 'screen', 'both', 'just', 'involved', 'madness', 'astonishing', 'animated', 'inner', 'struggle', 'adolescent', 'hero', '-', 'intense', 'herzog', 'alive', 'living', 'actress', 'complex', 'character', 'grows', 'shocking', 'part', 'charm', 'obvious', 'humour', 'son', 'too', 'sense', 'psychological', 'drama', 'burst', 'violence', 'startling', 'slow', 'intelligent', 'coming-of-age', 'journey', 'sensitive', 'young', 'girl', 'home', 'fierce', 'free', 'dangerous', 'mother', 'hold', 'over', 'truly', 'moving', 'experience', 'perfect', 'example', 'art', 'done', 'right', 'comfort', 'deeply', 'felt', 'stylized', 'triumph', 'director', 'deftly', 'suspense', 'whose', 'rather', 'substance', 'role', 'either', 'loving', 'your', 'loved', '!', 'constructed', 'were', 'ca', 'wait', 'see', 'happens', 'next', 'haneke', 'challenge', 'reality', 'sexual', 'absorbing', 'clarity', 'gone', 'against', 'should', 'painful', 'watch', 'viewer', 'chance', 'accomplished', 'riveting', 'depressing', 'view', 'iranian', 'close', 'imaginative', 'aside', 'narc', 'gritty', 'these', 'preposterous', 'thoroughly', 'incredibly', 'beautiful', 'look', 'o', 'dance', 'death', 'process', 'lot', 'hong', 'kong', 'kapur', 'flair', 'landscape', 'adventure', 'energy', 'animation', 'entire', 'sport', 'action', 'exciting', 'field', 'bourne', 'directs', 'location', 'us', 'ability', 'sincere', 'footage', 'horrifying', 'document', 'entertaining', 'child', 'create', 'history', 'powerful', 'detailed', 'significant', 'without', 'lift', 'above', 'level', 'hard', 'back', 'boy', 'nothing', 'promising', 'unusual', 'kind', 'horror', 'barely', 'sustain', 'discovery', 'channel', 'imax', 'clich√©s', 'perspective', 'up', 'photography', 'writer-director', 'fan', 'national', 'grief', 'curiosity', 'fear', 'enjoyed', 'favor', 'wa', 'quickly', 'memory', 'chicago', 'sophisticated', 'completely', 'execution', 'refreshing', 'five', 'female', 'high', 'school', 'friend', 'face', 'battle', 'try', 'relationship', 'deeper', 'water', 'surface', 'crime', 'flick', 'common', 'value', 'enterprise', 'previous', 'patience', 'under', 'possible', 'sequel', 'then', 'comedy', 'rule', 'own', '2', 'couple', 'mile', 'blue', 'ultimate', 'filled', 'pure', 'excitement', 'seen', 'mr', 'allows', 'finally', 'artist', 'feel-good', 'term', 'shot', 'style', 'subject', 'lacking', 'mostly', 'engrossing', 'resonant', 'secret', 'indeed', 'hip-hop', 'metaphor', 'ambition', 'dramatic', 'impact', 'holocaust', 'mainly', 'told', 'between', '?', 's', 'personal', 'revelation', 'big', 'result', 'gentle', 'somewhere', 'short', 'modern', 'neither', 'nor', 'spend', 'stand-up', 'very', 'world', 'unlike', 'tell', 'performer', 'key', 'intriguing', 'constantly', 'desire', 'man', 'format', 'manner', 'extreme', 'expectation', 'acting', 'dialogue', 'cinematography', 'pay', 'hear', 'news', 'human', 'first', 'quiet', 'otherwise', 'excellent', 'sequence', 'obviously', 'south', 'magic', 'walk', 'left', 'prove', 'drive', 'show', 'nature', 'theme', 'important', 'him', 'especially', 'finale', 'exactly', 'fair', 'coming', 'last', 'space', 'merely', 'technical', 'russian', 'cinematic', '[', ']', 'talented', 'terribly', 'charismatic', 'star', 'social', 'deal', 'spectacular', 'gem', 'obsession', 'delight', 'goofy', 'gory', 'stuff', 'cliche', 'potent', 'honesty', 'slasher', 'ticket', 'minor', 'balance', 'paint', 'sad', 'single', 'scene', 'special', 'effect', 'folk', 'force', 'humanity', 'george', 'forgotten', 'mike', 'win', 'originality', 'following', 'formula', 'winning', 'offering', 'audience', 'urban', 'welcome', 'half', 'giving', 'same', 'well-meaning', 'superior', 'played', 'boast', 'factor', 'allen', 'plenty', 'laugh', 'line', 'enjoyable', 'soul', 'likable', 'grasp', 'absurd', 'determined', 'entertain', 'lively', 'script', 'sharp', 'kiss', 'seem', 'must', 'jason', 'deserves', 'sister', 'husband', 'sweet', 'alabama', '\\x96', 'harmless', 'date', 'surprising', 'general', 'slowly', 'paced', 'thriller', 'michael', 'caine', 'faithful', 'premise', 'become', 'parent', 'age', 'precious', 'matter', 'posse', 'park', 'past', 'aspect', \"'ll\", 'visual', 'likely', 'anywhere', 'miss', 'humor', 'behavior', 'return', 'root', 'genre', 'well-made', 'certain', 'intelligence', 'morality', 'science', 'count', 'remains', 'solid', 'somewhat', 'heavy-handed', 'account', 'howard', 'hand', 'follows', 'realistic', 'path', 'uncompromising', 'existence', 'running', 'trapped', 'captivating', 'nair', 'complexity', 'family', 'shallow', 'attempt', 'indie', 'rank', 'among', 'flow', 'friendship', 'traditional', 'moviemaking', 'period', 'wit', 'maybe', 'release', 'worst', 'decade', 'honestly', 'analyze', 'bad', 'droll', 'well-acted', 'unexpected', 'feeling', 'huppert', 'cop', 'understands', 'medium', 'understated', 'jack', 'britney', 'delivered', 'safe', 'travel', 'outside', 'business', 'enjoy', 'mild', 'gang', 'put', 'video', 'spare', 'material', 'understand', \"'m\", 'sure', 'himself', 'good-natured', 'treat', 'slight', 'pleasure', 'insightful', 'side', 'conflict', 'say', 'another', 'bored', 'moment', 'heartbreaking', 'keep', 'queen', 'she', 'problem', 'natural', 'gift', 'john', 'english', 'terrific', 'dong', 'power', 'fact', 'leaving', 'interesting', 'wo', 'examination', 'joke', 'gripping', 'person', 'becomes', 'writing', 'wear', 'b-movie', 'striking', 'before', 'fully', 'admirable', 'early', 'mixed', 'emotion', 'understanding', 'tsai', 'taken', 'timely', 'foreign', 'rise', '20', 'production', 'escapism', 'study', 'hollywood', 'could', 'possibly', 'clumsy', 'equally', 'daring', 'mesmerizing', 'forget', 'painfully', 'moore', 'haynes', 'territory', 'brought', 'frame', 'touch', 'nerve', 'parker', 'brilliantly', 'source', 'hilarious', 'poem', 'sitcom', 'quite', 'delightful', 'thrilling', 'despite', 'flaw', 'least', 'amy', 'personality', 'polanski', 'typical', 'polished', 'christian', 'frailty', 'only', 'slightly', 'version', 'nearly', 'unflinching', 'lead', 'trip', 'particularly', 'passion', 'set', 'strange', 'filmed', 'taste', 'remarkably', 'accessible', 'affecting', 'buy', 'clooney', 'throughout', 'inventive', 'impressive', 'direction', 'narrative', 'spin', 'appreciate', 'intimate', 'large', 'grow', 'full', 'strength', 'warmth', 'believe', 'david', 'playing', 'storyline', 'bit', 'plot', 'development', 'touching', 'nostalgia', 'please', 'search', 'definitely', 'window', 'imagination', 'solondz', 'windtalker', 'pick', 'apart', 'admit', 'where', 'live', 'present', 'middle', 'musical', 'desperate', 'escape', 'anyway', 'difficult', 'call', 'benefit', 'remember', 'end', 'detail', 'nuance', 'speaks', 'event', 'easily', 'quirky', 'individual', 'figure', 'stand', 'chronicle', 'abuse', 'depiction', 'using', 'ship', 'continues', 'longer', 'generation', 'dumb', 'several', 'victim', 'none', 'williams', 'impossible', 'romance', 'pumpkin', 'dare', 'debut', 'first-time', 'considering', 'background', 'thought-provoking', 'increasingly', 'frightening', 'once', 'accent', 'emotional', 'sit', 'success', 'element', 'directorial', 'wilde', 'absolutely', 'meaning', 'anything', 'fantasy', 'attitude', 'push', 'message', 'trouble', 'every', 'seems', 'me', 'fashion', 'ingredient', 'interview', 'cloying', 'wonderful', 'talk', 'woman', 'offbeat', 'fat', 'greek', 'stirring', 'godard', 'image', 'add', 'fine', 'change', 'necessary', 'politics', 'looking', 'energetic', 'largely', \"'60s\", 'interested', 'grace', 'read', 'disappointed', 'guy', 'brain', 'exploration', 'writer', 'given', 'caper', 'twist', 'worthy', 'thinking', 'uplifting', 'sentimental', 'usual', 'brilliant', 'pretty', 'soderbergh', 'frontal', 'did', 'stop', 'war', 'course', 'surreal', 'french', 'simple', 'taking', 'choose', 'depth', 'thing', 'predictable', 'italian', 'based', 'keeping', 'melodrama', 'emotionally', 'thanks', 'credible', 'delivers', 'able', 'theater', 'spielberg', 'philosophical', 'bold', 'lyrical', 'pat', 'notion', 'respect', 'wild', 'again', 'build', 'complete', 'hope', 'satisfying', 'sort', 'food', 'brutal', 'strangely', 'fiction', 'cultural', 'remarkable', 'meditation', 'revolution', 'follow', 'creepy', 'committed', 'involving', 'chinese', 'wind', 'acted', 'actual', 'price', 'intellectual', 'weak', 'revenge', 'lady', 'subtle', 'mix', 'serious', 'jealousy', 'within', 'seemingly', 'marriage', 'proceeding', 'pianist', 'transcends', 'office', 'getting', 'probably', 'appeal', 'tough', 'shake', 'expected', 'contrivance', 'political', 'damn', 'over-the-top', 'got', 'poignant', 'reveals', 'learn', 'nicely', 'serf', 'society', 'heartfelt', 'quest', 'sexy', 'gorgeous', 'sex', 'dahmer', 'expect', 'bunch', 'poetry', 'check', 'particular', 'interest', 'student', 'designed', 'major', 'report', 'truth', 'small', 'amount', 'wannabe', 'written', 'produced', 'directed', 'authentic', 'affection', 'monster', 'liked', 'hill', 'jones', 'buff', 'recommend', 'historical', 'alone', 'hey', 'vivid', 'although', 'cliched', 'raise', 'maudlin', 'stunning', 'visuals', 'pretension', 'prof', 'breathtaking', 'strike', 'menace', 'vibrant', 'focus', 'upon', 'successful', 'slick', 'witty', 'deserve', 'leave', 'building', 'until', 'break', 'job', 'note', 'use', 'near', 'hour', 'flash', 'kaufman', 'creates', 'night', 'killed', 'niro', 'murphy', 'sensibility', 'nonsense', 'represents', 'demand', 'tie', 'toward', 'talent', 'wry', 'pathos', 'virtue', 'dark', 'surprisingly', 'stick', 'refuse', 'entirely', 'hip', 'delivery', 'relatively', 'elegant', 'having', 'slapstick', 'silly', 'street', 'wonder', 'ghost', 'themselves', 'sly', 'others', 'becoming', 'men', 'logic', 'after', 'wondering', 'happen', 'pretentious', 'three', 'c', 'h', 'vulgar', 'inspiring', 'camp', 'terrible', 'future', 'cult', 'classic', 'crowd', 'form', 'frida', 'finest', 'dry', 'stay', 'clear', 'yourself', 'festival', 'producer', 'each', 'delicate', 'bloody', 'visceral', 'language', 'fantastic', 'theatre', 'featuring', 'identity', 'lee', 'brown', 'saga', 'filmmaking', 'soon', 'bland', 'x', 'cheesy', 'vividly', 'flawed', 'test', 'sick', 'twisted', 'tour', 'below', 'unpleasant', 'realize', 'fisher', 'fly', 'spy', 'visually', 'funnier', 'spot', 'sandler', 'watchable', 'reno', 'during', 'predictably', 'heartwarming', 'desperately', 'madonna', 'suck', 'second', 'adult', 'seven', 'challenging', 'equal', 'innocence', 'rarely', 'along', 'colorful', 'farce', 'type', 'deliver', 'further', 'inspired', 'cruel', 'fate', 'thousand', 'tragedy', 'crazy', 'sink', 'exploitation', 'grand', 'memorable', 'average', 'broad', 'tear', 'catch', 'warm', 'realized', 'observation', 'mood', 'rich', 'suspenseful', 'moral', 'casting', 'sheer', 'predecessor', 'summer', 'used', 'provide', 'exploit', 'reggio', 'leaf', 'purpose', 'setting', 'went', 'embrace', 'zhang', 'lie', 'genuinely', 'bring', 'happiness', 'skill', 'smith', 'yes', 'goal', 'manipulative', 'appealing', 'fast', 'poetic', 'chilling', 'sight', 'asks', 'faith', 'washington', 'hopkins', 'relentless', 'creation', 'guilty', 'degree', 'gross-out', 'current', 'inherent', 'nuanced', 'enthusiasm', 'achieves', 'blend', 'telling', 'era', 'meet', 'uneven', 'homage', 'remake', 'pack', 'creative', 'connection', 'ya-ya', 'corner', 'club', 'atmosphere', 'spirited', 'ingenious', 'earnest', 'courage', 'conviction', 'class', 'editing', 'closer', 'deft', 'gradually', 'diverting', 'profound', 'spiritual', 'bizarre', 'produce', 'joy', 'fit', 'inside', 'holiday', 'admire', 'old-fashioned', 'explores', 'crisis', 'compassion', 'terrifying', 'carefully', 'unsettling', 'quietly', 'budget', 'affair', 'generally', 'providing', 'suggests', 'tragic', 'scary', 'found', 'ground', 'intrigue', 'murder', 'presence', 'reasonably', 'beyond', 'wall', 'skip', 'opportunity', 'busy', 'wanted', 'actually', 'cut', 'ice', 'weird', 'resonance', 'group', 'equivalent', 'dig', 'deep', 'miller', 'shine', 'remembered', 'oscar', 'portrait', 'soldier', 'onscreen', 'kevin', 'fill', 'marvel', 'cartoon', 'instead', 'wrong', 'halfway', 'showcase', 'promise', 'overcome', 'utter', 'stuart', 'week', 'awkward', 'extraordinary', 'formulaic', 'record', 'drawn', 'satire', 'skin', 'lost', 'wife', 'literary', 'dazzling', 'dancing', 'share', 'fairly', 'cho', 'appears', 'woo', 'drop', 'modest', 'breezy', 'conventional', 'bitter', 'black', 'delightfully', 'finish', 'loose', 'alien', 'raw', 'haunting', 'funniest', 'hook', 'y', 'romp', 'vehicle', 'dream', 'pokemon', 'franchise', 'opening', 'credit', 'television', 'diversion', 'christmas', 'repetitive', 'adaptation', 'shadow', 'revealing', 'richly', 'final', 'roger', 'trying', 'humorous', 'clearly', 'reach', 'una', 'sea', \"'ve\", 'wonderfully', 'cynical', 'odd', 'screenwriter', 'match', 'contrived', 'mistake', 'worthwhile', 'thrill', 'sum', 'supposed', 'subversive', 'across', 'speaking', 'wise', '*', 'fight', 'explosion', 'order', 'created', 'wacky', 'device', 'tap', 'universal', 'public', 'robin', 'string', 'sappy', 'core', 'artistic', 'childhood', 'mess', 'scenario', 'tribute', 'inspire', 'consistently', 'bright', 'amused', 'jump', 'house', 'bittersweet', 'moviegoer', 'answer', 'highly', 'built', 'afraid', 'front', 'timing', 'stupid', 'straight', 'nowhere', 'critic', 'conversation', 'machine', 'design', 'central', 'loss', 'portrayal', 'subtlety', 'commercial', 'roll', 'un', 'display', 'thought', 'chemistry', 'endearing', 'stock', 'concerned', 'member', 'voice', 'false', 'step', 'latest', 'considerable', 'company', 'self-conscious', 'unfolds', 'tasty', 'cautionary', 'davis', 'male', 'approach', 'storytelling', 'japanese', 'sour', 'entry', 'broomfield', 'eight', 'vital', 'spite', 'scorsese', 'camera', 'gifted', 'king', 'except', 'outrageous', 'monty', 'fails', 'engage', 'mildly', 'saturday', 'matinee', 'spark', 'packed', 'community', 'choice', 'father', 'total', 'name', 'southern', 'usually', 'contemporary', 'relies', 'city', 'unless', 'fighting', 'blow', 'market', 'eventually', 'lesson', 'wildly', 'prison', 'persona', 'maker', 'foot', 'angst', 'sci-fi', 'hybrid', 'ridiculous', 'blade', 'ii', 'competent', 'charlie', 'carry', 'handsome', 'dead', 'starring', 'blood', 'virtually', 'b', 'soundtrack', 'vein', 'situation', 'turning', 'pop', 'junk', 'slap', 'motion', 'car', 'throw', 'context', 'hopeful', 'award', 'bag', 'sentimentality', 'team', 'decent', 'domestic', 'shame', 'party', 'reign', 'fire', 'angel', 'episode', 'pity', 'viewing', 'popcorn', 'stage', 'fall', 'harry', 'steal', 'dragon', 'scenery', 'showing', 'control', 'treatment', 'player', 'convincing', 'irony', 'scare', 'teenager', 'service', 'heaven', 'necessarily', 'state', 'zone', 'possibility', 'cool', 'leigh', 'villain', 'lane', 'hardly', 'poignancy', 'piano', 'teacher', 'gag', 'consider', 'real-life', 'seat', 'glory', 'ago', 'appear', 'buddy', 'chris', 'sade', 'sentiment', 'achievement', 'violent', 'hoffman', 'private', 'warning', 'rewarding', 'smug', 'derivative', 'empathy', 'spectacle', 'nasty', 'band', 'destination', 'color', 'lawrence', 'devastating', 'manage', 'rap', 'cold', 'industry', 'crafted', 'absurdity', 'protagonist', 'austin', 'smarter', 'spoof', 'woody', 'land', 'unintentionally', 'winner', 'doubt', 'list', 't', 'studio', 'stylish', 'damned', 'said', 'addition', 'growing', 'ryan', 'let', 'consequence', 'writer/director', 'cover', 'mixture', 'characterization', 'empty', '2002', 'lover', 'beneath', 'shooting', 'assured', 'hollow', 'despair', 'bottom', 'um', 'em', 'shyamalan', 'costume', 'ambitious', 'tv', 'suffers', 'concept', 'drag', 'pain', 'wilson', 'save', 'avoid', 'turned', 'potentially', 'chuckle', 'contains', 'reaction', 'paul', 'bother', 'preachy', 'already', 'reminds', 'robert', 'frustrating', 'oddly', 'stomach', 'content', 'ugly', 'unlikely', 'topic', 'tedious', 'disappointing', 'chan', 'kissinger', 'aim', 'sympathy', 'eastwood', 'killer', 'capable', 'third', 'finding', 'god', 'tension', 'red', 'blair', 'witch', 'trick', 'gay', 'justice', 'white', 'trash', 'collateral', 'damage', 'awful', 'quick', 'v', 'xxx', 'overlong', 'belongs', 'sadness', 'theatrical', 'waiting', 'filme', 'length', 'creativity', 'copy', 'opera', 'martin', 'weight', 'nicholas', 'trek', 'country', 'low-budget', 'missing', 'artificial', 'structure', 'nightmare', 'overall', \"'70s\", 'room', 'body', 'mad', 'exercise', 'hot', 'difference', 'tom', 'impression', 'constant', 'dramatically', 'excess', 'dynamic', 'rent', 'imagine', 'twice', 'knowing', 'revelatory', 'evil', 'dealing', 'intensity', 'money', 'released', 'delivering', 'directing', 'college', 'noir', 'harris', 'chase', 'pic', 'palma', 'candy', 'james', 'birthday', 'trifle', 'smile', 'intoxicating', 'onto', 'due', 'parable', 'stereotype', 'wave', 'number', 'essentially', 'rip-off', 'chill', 'punch', 'hole', 'waste', 'teenage', 'complicated', 'failure', 'condition', 'million', 'inept', 'encounter', 'tradition', 'low', 'yu', 'downright', 'bigger', 'mainstream', 'demonstrates', 'heroine', 'digital', 'muddled', 'myself', 'cause', 'green', 'silliness', 'mention', 'saw', 'sign', 'wish', 'con', 'combine', 'testament', 'shock', 'store', 'succeeds', 'frank', 'realism', 'strain', 'season', 'pas', 'heard', \"'the\", 'collection', 'notice', 'laughter', 'saccharine', 'unfaithful', 'trapping', 'soap', 'silence', 'serving', 'whimsical', 'saying', 'freak', 'thin', 'center', 'dirty', 'comedic', 'seriously', 'exploitative', 'movement', 'credibility', 'basic', 'brutally', 'imitation', 'york', 'started', 'main', 'gun', 'boring', 'worse', 'superb', 'distance', 'stuck', '$', 'dysfunctional', 'limited', 'late', 'happened', '10', 'somebody', 'spider-man', 'mindless', 'sitting', 'unfocused', 'dull', 'strictly', 'disaster', 'burn', 'irritating', 'endeavor', 'en', 'believable', 'earth', 'friday', 'excuse', 'nevertheless', 'older', 'spell', 'barry', 'frequently', 'overwrought', 'various', 'retread', 'rush', 'somber', 'cross', 'abstract', 'knowledge', 'indian', 'manhattan', 'bear', 'executed', 'mayhem', 'spent', 'required', 'broken', 'graphic', 'thick', 'attraction', 'needed', 'gangster', 'die', 'barbershop', 'supporting', 'embarrassment', 'inoffensive', 'including', 'basically', 'fancy', 'changing', 'tired', 'wallace', 'meandering', 'flashy', 'lame', 'dreary', 'hackneyed', 'animal', 'plain', 'vampire', 'apparently', 'clich√©', 'forgive', 'halloween', 'shoot', 'stale', 'served', 'pacing', 'computer', 'scratch', 'apparent', 'mediocre', 'screenwriting', 'inspiration', 'saving', 'sugar', 'layer', 'sloppy', 'mere', 'professional', 'del', 'totally', 'ask', 'slip', 'forced', 'commentary', 'simplistic', 'confusing', 'conclusion', '9', 'amateurish', 'disbelief', 'horrible', '90', 'obnoxious', 'superficial', 'banal', 'chaotic', 'ludicrous', 'biting', 'ordinary', 'da', 'harvard', 'settle', 'annoying', 'relentlessly', 'routine', 'merit', 'devoid', 'parody', 'seek', 'lazy', 'ritchie', 'meant', 'crude', 'witless', 'self-indulgent', 'biggest', 'worked', 'reference', 'kung', 'pow', 'staged', 'pathetic', 'santa', 'showtime', 'trite', 'wasted', 'suspect', 'generic', 'poor', 'justify', 'disjointed', 'endless', 'fatal', 'generate', 'after-school', 'loud', 'sadly', 'flat', 'missed', 'unintentional', 'plotting', 'kill', 'pile', 'offensive', 'feature-length', 'pretend', 'tiresome', 'schneider', 'product', 'comparison', 'incoherent', 'ballistic', 'seagal', 'badly', 'pointless', 'plodding', 'unfunny', 'lousy', 'choppy', 'stealing', 'tuxedo', 'poorly', 'crap', 'leaden', 'carvey', 'lifeless', '/', 'uninspired', 'bore', 'disguise', 'sara', 'inane', 'soggy', 'unnecessary', 'benigni', 'pinocchio', '51']\n"
     ]
    }
   ],
   "source": [
    "\tlexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "\tw_counts = Counter(lexicon)\n",
    "\tl2 = []\n",
    "\tfor w in w_counts:\n",
    "\t\t#print(w_counts[w])\n",
    "\t\tif 1000 > w_counts[w] > 9:\n",
    "\t\t\tl2.append(w)\n",
    "\tprint(l2)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(pos,neg):\n",
    "\n",
    "\tlexicon = []\n",
    "\twith open(pos,'r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)\n",
    "            \n",
    "\twith open(neg,'r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tall_words = word_tokenize(l)\n",
    "\t\t\tlexicon += list(all_words)\n",
    "\n",
    "\tlexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "\tw_counts = Counter(lexicon)\n",
    "\tl2 = []\n",
    "\tfor w in w_counts:\n",
    "\t\t#print(w_counts[w])\n",
    "\t\tif 1000 > w_counts[w] > 50:\n",
    "\t\t\tl2.append(w)\n",
    "\tprint((l2))\n",
    "\treturn l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', '``', 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n",
      "230193\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "[2. 1. 1. ... 0. 0. 0.]\n",
      "['the', 'gorgeously', 'elaborate', 'continuation', 'of', '``', 'the', 'lord', 'of', 'the', 'rings', '``', 'trilogy', 'is', 'so', 'huge', 'that', 'a', 'column', 'of', 'words', 'can', 'not', 'adequately', 'describe', 'co-writer/director', 'peter', 'jackson', \"'s\", 'expanded', 'vision', 'of', 'j', '.', 'r', '.', 'r', '.', 'tolkien', \"'s\", 'middle-earth', '.']\n",
      "230193\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "[3. 0. 1. ... 0. 0. 0.]\n",
      "['effective', 'but', 'too-tepid', 'biopic']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "['if', 'you', 'sometimes', 'like', 'to', 'go', 'to', 'the', 'movies', 'to', 'have', 'fun', ',', 'wasabi', 'is', 'a', 'good', 'place', 'to', 'start', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "['emerges', 'as', 'something', 'rare', ',', 'an', 'issue', 'movie', 'that', \"'s\", 'so', 'honest', 'and', 'keenly', 'observed', 'that', 'it', 'does', \"n't\", 'feel', 'like', 'one', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "['the', 'film', 'provides', 'some', 'great', 'insight', 'into', 'the', 'neurotic', 'mindset', 'of', 'all', 'comics', '--', 'even', 'those', 'who', 'have', 'reached', 'the', 'absolute', 'top', 'of', 'the', 'game', '.']\n",
      "230193\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "['offers', 'that', 'rare', 'combination', 'of', 'entertainment', 'and', 'education', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "['perhaps', 'no', 'picture', 'ever', 'made', 'has', 'more', 'literally', 'showed', 'that', 'the', 'road', 'to', 'hell', 'is', 'paved', 'with', 'good', 'intentions', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "[1. 0. 1. ... 0. 0. 0.]\n",
      "['steers', 'turns', 'in', 'a', 'snappy', 'screenplay', 'that', 'curls', 'at', 'the', 'edges', ';', 'it', \"'s\", 'so', 'clever', 'you', 'want', 'to', 'hate', 'it', '.', 'but', 'he', 'somehow', 'pulls', 'it', 'off', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "['take', 'care', 'of', 'my', 'cat', 'offers', 'a', 'refreshingly', 'different', 'slice', 'of', 'asian', 'cinema', '.']\n",
      "230193\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a7a3fba1ad5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mfeatureset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "featureset = []\n",
    "\n",
    "with open('pos.txt','r') as f:\n",
    "    contents = f.readlines()\n",
    "    \n",
    "    for l in contents[:10]:\n",
    "        current_words = word_tokenize(l.lower())\n",
    "        print(current_words)\n",
    "        current_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "        features = np.zeros(len(lexicon))\n",
    "        print(len(features))\n",
    "        for word in current_words:\n",
    "            if word.lower() in lexicon:\n",
    "                index_value = lexicon.index(word.lower())\n",
    "                features[index_value] += 1\n",
    "                print(features)\n",
    "            else:\n",
    "                print(\"s\")\n",
    "                \n",
    "        features = list(features)\n",
    "        featureset.append([features,[1,0]])\n",
    "print((featureset[1][2]))\n",
    "       \n",
    "        \n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_handling(sample,lexicon,classification):\n",
    "\n",
    "\tfeatureset = []\n",
    "\n",
    "\twith open('sample','r') as f:\n",
    "\t\tcontents = f.readlines()\n",
    "\t\tfor l in contents[:hm_lines]:\n",
    "\t\t\tcurrent_words = word_tokenize(l.lower())\n",
    "\t\t\tcurrent_words = [lemmatizer.lemmatize(i) for i in current_words]\n",
    "\t\t\tfeatures = np.zeros(len(lexicon))\n",
    "\t\t\tfor word in current_words:\n",
    "\t\t\t\tif word.lower() in lexicon:\n",
    "\t\t\t\t\tindex_value = lexicon.index(word.lower())\n",
    "\t\t\t\t\tfeatures[index_value] += 1\n",
    "\n",
    "\t\t\tfeatures = list(features)\n",
    "\t\t\tfeatureset.append([features,classification])\n",
    "\n",
    "\treturn featureset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n",
    "\tlexicon = create_lexicon(pos,neg)\n",
    "\tfeatures = []\n",
    "\tfeatures += sample_handling('pos.txt',lexicon,[1,0])\n",
    "\tfeatures += sample_handling('neg.txt',lexicon,[0,1])\n",
    "    \n",
    "\trandom.shuffle(features)\n",
    "\tfeatures = np.array(features)\n",
    "\n",
    "\ttesting_size = int(test_size*len(features))\n",
    "\n",
    "\ttrain_x = list(features[:,0][:-testing_size])\n",
    "\ttrain_y = list(features[:,1][:-testing_size])\n",
    "\ttest_x = list(features[:,0][-testing_size:])\n",
    "\ttest_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "\treturn train_x,train_y,test_x,test_y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'new', '``', 'he', 'going', 'make', 'even', 'than', 'or', 'so', 'can', 'not', 'if', 'sometimes', 'like', 'go', 'have', 'fun', 'good', 'place', 'start', 'something', 'rare', 'doe', \"n't\", 'feel', 'one', 'some', 'great', 'into', 'all', 'comic', '--', 'those', 'who', 'game', 'offer', 'entertainment', 'no', 'picture', 'ever', 'made', 'ha', 'more', 'turn', 'screenplay', 'at', ';', 'clever', 'want', 'off', 'take', 'care', 'my', 'cinema', 'well', 'worth', 'what', 'really', 'surprise', 'about', '(', ')', 'why', 'we', ':', 'through', 'eye', 'heart', 'mind', 'ultimately', 'reason', 'need', 'story', 'much', 'compelling', \"'\", 'which', 'most', 'come', 'documentary', 'year', 'making', 'will', 'fresh', 'true', 'cast', 'actor', 'u', 'because', 'amusing', 'how', 'our', 'life', 'music', 'by', 'everyone', 'from', 'few', 'point', 'also', 'love', 'out', 'romantic', 'minute', 'familiar', 'lack', 'often', 'tale', 'effort', 'tone', 'down', 'his', 'play', 'real', 'kid', 'anyone', 'filmmaker', 'there', 'way', 'i', 'think', 'project', 'while', 'would', 'easy', 'give', 'title', 'two', 'far', 'any', 'though', 'everything', 'might', 'smart', 'never', 'always', 'day', 'almost', 'her', 'best', 'work', 'fascinating', 'had', 'them', 'are', 'they', 'on', 'being', 'series', 'around', 'better', 'ending', 'le', 'time', 'de', 'strong', 'itself', 'long', 'little', 'away', 'performance', 'else', 'people', 'culture', 'do', 'know', 'yet', 'funny', 'may', 'old', 'feature', 'their', 'get', 'watching', 'engaging', \"'d\", 'book', 'such', 'beautifully', 'run', 'find', 'manages', 'original', 'many', 'idea', 'hit', 'simply', 'whole', 'certainly', 'spirit', 'piece', 'now', 'enough', 'still', 'charming', 'here', 'whether', \"'re\", 'other', 'together', 'american', 'since', 'been', 'when', 'screen', 'both', 'just', '-', 'character', 'part', 'charm', 'obvious', 'too', 'sense', 'drama', 'intelligent', 'young', 'girl', 'home', 'over', 'moving', 'experience', 'art', 'done', 'right', 'director', 'whose', 'rather', 'role', 'either', 'your', '!', 'were', 'ca', 'see', 'next', 'should', 'watch', 'viewer', 'these', 'beautiful', 'look', 'lot', 'adventure', 'energy', 'action', 'entertaining', 'child', 'history', 'powerful', 'without', 'level', 'hard', 'back', 'boy', 'nothing', 'kind', 'horror', 'up', 'fan', 'wa', 'completely', 'high', 'face', 'try', 'relationship', 'crime', 'flick', 'under', 'sequel', 'then', 'comedy', 'own', 'seen', 'mr', 'shot', 'style', 'subject', 'mostly', 'between', '?', 'big', 'short', 'modern', 'very', 'world', 'tell', 'man', 'acting', 'dialogue', 'human', 'first', 'show', 'him', 'especially', 'last', 'cinematic', '[', ']', 'star', 'stuff', 'cliche', 'scene', 'special', 'effect', 'audience', 'half', 'same', 'laugh', 'line', 'enjoyable', 'script', 'seem', 'sweet', 'thriller', 'premise', 'become', 'matter', 'past', \"'ll\", 'visual', 'likely', 'humor', 'genre', 'solid', 'family', 'attempt', 'wit', 'worst', 'bad', 'feeling', 'enjoy', 'put', 'video', 'material', 'sure', 'pleasure', 'say', 'another', 'moment', 'keep', 'she', 'problem', 'john', 'power', 'fact', 'interesting', 'wo', 'joke', 'becomes', 'before', 'production', 'study', 'hollywood', 'could', 'quite', 'despite', 'least', 'only', 'version', 'nearly', 'lead', 'set', 'direction', 'narrative', 'full', 'bit', 'plot', 'where', 'end', 'quirky', 'romance', 'debut', 'once', 'emotional', 'anything', 'message', 'every', 'seems', 'me', 'woman', 'image', 'fine', 'looking', 'guy', 'pretty', 'did', 'war', 'french', 'thing', 'predictable', 'melodrama', 'theater', 'again', 'sort', 'probably', 'interest', 'directed', 'although', 'leave', 'hour', 'talent', 'dark', 'surprisingly', 'silly', 'men', 'after', 'three', 'classic', 'each', 'along', 'actually', 'portrait', 'instead', 'trying', \"'ve\", 'mess', 'fall', 'gag', 'exercise', 'boring', 'dull']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-59b08577fc97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_feature_sets_and_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neg.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8524fd5d03da>\u001b[0m in \u001b[0;36mcreate_feature_sets_and_labels\u001b[0;34m(pos, neg, test_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msample_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neg.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-44959f5e67f1>\u001b[0m in \u001b[0;36msample_handling\u001b[0;34m(sample, lexicon, classification)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                 \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mhm_lines\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample'"
     ]
    }
   ],
   "source": [
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_sentiment_featuresets import create_feature_sets_and_labels\n",
    "import tensorflow as tf\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','/path/neg.txt')\n",
    "\n",
    "n_nodes_hl1 = 1500\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 1500\n",
    "\n",
    "n_classes = 2\n",
    "batch_size = 100\n",
    "hm_epochs = 10\n",
    "\n",
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(train_x[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}\n",
    "\n",
    "\n",
    "# Nothing changes\n",
    "def neural_network_model(data):\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_neural_network(x):\n",
    "\tprediction = neural_network_model(x)\n",
    "\tcost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.initialize_all_variables())\n",
    "\t    \n",
    "\t\tfor epoch in range(2):\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\ti=0\n",
    "\t\t\twhile i < len(train_x):\n",
    "\t\t\t\tstart = i\n",
    "\t\t\t\tend = i+batch_size\n",
    "\t\t\t\tbatch_x = np.array(train_x[start:end])\n",
    "\t\t\t\tbatch_y = np.array(train_y[start:end])\n",
    "\n",
    "\t\t\t\t_, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "\t\t\t\t                                              y: batch_y})\n",
    "\t\t\t\tepoch_loss += c\n",
    "\t\t\t\ti+=batch_size\n",
    "\t\t\t\t\n",
    "\t\t\tprint('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\t\tcorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\t\taccuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "\t\tprint('Accuracy:',accuracy.eval({x:test_x, y:test_y}))\n",
    "\n",
    "\t    \n",
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\ttrain_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "\t# if you want to pickle this data:\n",
    "\twith open('/path/to/sentiment_set.pickle','wb') as f:\n",
    "\t\tpickle.dump([train_x,train_y,test_x,test_y],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
